{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Transition-based Neural Network Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chen and Manning (2014) [A Fast and Accurate Dependency Parser using Neural Networks](https://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf)\n",
    "* CS224n Assignment #2 (Winter 2018) [Question 2](http://web.stanford.edu/class/cs224n/assignment2/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tempfile import gettempdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for downloading and processing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penn Treebank-3 (https://catalog.ldc.upenn.edu/ldc99t42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, filename, expected_bytes):\n",
    "    \"Download the file if not present, make sure it's the right size.\"    \n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n",
    "        statinfo = os.stat(local_filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            with zipfile.ZipFile(local_filename) as f:\n",
    "                f.extractall()\n",
    "        else:\n",
    "            print(statinfo.st_size)\n",
    "            raise Exception('Failed to verify ' + local_filename + \n",
    "                            '. Can you get to it with a browser?')\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(in_file, lowercase=False, max_example=None):\n",
    "    \"\"\" Returns a list of examples where each example is a dict of lists:\n",
    "        'word' : list of str repr. the words in the sentence\n",
    "        'pos'  : list of str repr. the XPOS tags (language specific)\n",
    "        'head' : list of int repr. the position of each word's head word\n",
    "        'label': list of str repr. the dependency label of each word\"\"\"\n",
    "    examples = []\n",
    "    with open(in_file) as f:\n",
    "        word, pos, head, label = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            sp = line.strip().split('\\t')\n",
    "            if len(sp) == 10:\n",
    "                if '-' not in sp[0]:\n",
    "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
    "                    pos.append(sp[4])\n",
    "                    head.append(int(sp[6]))\n",
    "                    label.append(sp[7])\n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'label': label})\n",
    "                word, pos, head, label = [], [], [], []\n",
    "                if (max_example is not None) and (len(examples) == max_example):\n",
    "                    break\n",
    "        if len(word) > 0:\n",
    "            examples.append({'word': word, 'pos': pos,\n",
    "                             'head': head, 'label': label})\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(keys, n_max=None, offset=0):\n",
    "    \"Helper function for building the mapping dicts\"\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    ls = count.most_common() if n_max is None \\\n",
    "        else count.most_common(n_max)\n",
    "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct(language, pos):\n",
    "    if language == 'english':\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    elif language == 'universal':\n",
    "        return pos == 'PUNCT'\n",
    "    else:\n",
    "        raise ValueError('language: %s is not supported.' % language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(reduced=True):\n",
    "    config = ParserConfig()\n",
    "    print(\"Downloading data...\")\n",
    "    start = time.time()\n",
    "    zipfile = maybe_download('http://web.stanford.edu/class/cs224n/assignment2/',\n",
    "                             'assignment2.zip',38866004)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "    \n",
    "    print (\"Loading data...\"),\n",
    "    start = time.time()\n",
    "    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n",
    "                           lowercase=config.lowercase)\n",
    "    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n",
    "                         lowercase=config.lowercase)\n",
    "    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n",
    "                          lowercase=config.lowercase)\n",
    "    if reduced:\n",
    "        train_set = train_set[:1000]\n",
    "        dev_set = dev_set[:500]\n",
    "        test_set = test_set[:500]\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Building parser...\")\n",
    "    start = time.time()\n",
    "    parser = Parser(train_set)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Loading pretrained embeddings...\")\n",
    "    start = time.time()\n",
    "    word_vectors = {}\n",
    "    for line in open(os.path.join(config.data_path, config.embedding_file)):\n",
    "        sp = line.strip().split()\n",
    "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "    for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Vectorizing data...\")\n",
    "    start = time.time()\n",
    "    train_set = parser.vectorize(train_set)\n",
    "    dev_set = parser.vectorize(dev_set)\n",
    "    test_set = parser.vectorize(test_set)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Preprocessing training data...\")\n",
    "    start = time.time()\n",
    "    train_examples = parser.create_instances(train_set)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    return parser, embeddings_matrix, train_examples, dev_set, test_set,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for Dependency Parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "        pred = self.parser.model.predict_on_batch(self.parser.session, mb_x)\n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_PREFIX = '<p>:'\n",
    "L_PREFIX = '<l>:'\n",
    "UNK = '<UNK>'\n",
    "NULL = '<NULL>'\n",
    "ROOT = '<ROOT>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    \"Contains everything needed for transition-based dependency parsing except for the model\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        # Check that there is a unique label for root\n",
    "        root_labels = list([l for ex in dataset\n",
    "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
    "        counter = Counter(root_labels)\n",
    "        if len(counter) > 1:\n",
    "            print('Warning: more than one root label')\n",
    "            print(counter)\n",
    "        self.root_label = counter.most_common()[0][0]\n",
    "        \n",
    "        # list of all unique dependency labels\n",
    "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
    "                                               for w in ex['label']\n",
    "                                               if w != self.root_label]))\n",
    "        \n",
    "        # DEP labels such as <l>:acl, ... , <l>:xcomp, <l>:<NULL>\n",
    "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
    "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
    "\n",
    "        config = ParserConfig()\n",
    "        self.unlabeled = config.unlabeled\n",
    "        self.with_punct = config.with_punct\n",
    "        self.use_pos = config.use_pos\n",
    "        self.use_dep = config.use_dep\n",
    "        self.language = config.language\n",
    "        \n",
    "        # dictionaries for transitions (left and right) incl. S for shift\n",
    "        if self.unlabeled:\n",
    "            trans = ['L', 'R', 'S']\n",
    "            self.n_deprel = 1\n",
    "        else: \n",
    "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
    "            self.n_deprel = len(deprel)\n",
    "\n",
    "        self.n_trans = len(trans) # number of unique arc-transitions\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "        \n",
    "        # POS tags such as <p>:$, ... , <p>:<UNK>, <p>:<NULL>, <p>:<ROOT>\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "        \n",
    "        # Words including <UNK>, <NULL>, <ROOT>\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK] = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "\n",
    "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
    "        self.n_tokens = len(tok2id)\n",
    "\n",
    "        \n",
    "    def vectorize(self, examples):\n",
    "        \" Numericalizes examples and adds ROOT to the front\"\n",
    "        vec_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['label']]\n",
    "            vec_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'label': label})\n",
    "        return vec_examples\n",
    "\n",
    "    \n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "        \"Returns list of features described in sec 3.1 of Chen and Manning\"\n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0\n",
    "\n",
    "        def get_lc(k):\n",
    "            # sorted indices of all left children of word at position k \n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        def get_rc(k):\n",
    "            # sorted indices of all right children of word at position k\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "        p_features = []\n",
    "        l_features = []\n",
    "        # top 3 words on the stack (s1, s2, s3)\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        # top 3 words in the buffer\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        if self.use_pos: # add POS tags for top 3 words on stack and in buffer\n",
    "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "\n",
    "        # first and second leftmost/rightmost children of top 2 words on stack\n",
    "        for i in range(2):\n",
    "            if i < len(stack):   # check that there are sufficient words on stack\n",
    "                k = stack[-i-1]  # index into top 2 words on stack\n",
    "                lc = get_lc(k)   # all left children\n",
    "                rc = get_rc(k)   # all right children\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else [] # left children of leftmost child\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else [] # right children of rightmost child\n",
    "                # first and second leftmost/rightmost children\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                # leftmost child of leftmost child\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                # rightmost child of rightmost child\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                if self.use_pos:\n",
    "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "\n",
    "                if self.use_dep:\n",
    "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
    "            else:\n",
    "                features += [self.NULL] * 6\n",
    "                if self.use_pos:\n",
    "                    p_features += [self.P_NULL] * 6\n",
    "                if self.use_dep:\n",
    "                    l_features += [self.L_NULL] * 6\n",
    "\n",
    "        features += p_features + l_features\n",
    "        assert len(features) == self.n_features\n",
    "        return features\n",
    "    \n",
    "\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        \"\"\" Implements 'shortest stack oracle' in sec 3.2 of Chen and Manning\n",
    "            Returns:\n",
    "            (1) 2 (if unlabeled) or index for S (shift operation)\n",
    "            (2) 0 (if unlabeled) or DEP label of second word on stack, s2 (left-arc)\n",
    "            (3) 1 (if unlabeled) or DEP label of first word on stack, s1 (right arc)\n",
    "        \"\"\"\n",
    "        if len(stack) < 2:            # stack contans only one word\n",
    "            return self.n_trans - 1   # return shift operation\n",
    "\n",
    "        i0 = stack[-1]                # position of top two words on stack \n",
    "        i1 = stack[-2]\n",
    "        h0 = ex['head'][i0]           # position of their respective heads\n",
    "        h1 = ex['head'][i1]\n",
    "        l0 = ex['label'][i0]          # their respective dependency labels\n",
    "        l1 = ex['label'][i1]\n",
    "\n",
    "        if self.unlabeled:\n",
    "            if (i1 > 0) and (h1 == i0):  # second on stack is not root  \n",
    "                return 0                 # second on stack is dep on first\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return 1\n",
    "            else:\n",
    "                return None if len(buf) == 0 else 2\n",
    "        else:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
    "            else:\n",
    "                return None if len(buf) == 0 else self.n_trans - 1\n",
    "\n",
    "            \n",
    "    def create_instances(self, examples):\n",
    "        \"\"\n",
    "        all_instances = []\n",
    "        succ = 0\n",
    "        for id, ex in enumerate(examples):\n",
    "            n_words = len(ex['word']) - 1\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            for i in range(n_words * 2):\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                legal_labels = self.legal_labels(stack, buf)\n",
    "                assert legal_labels[gold_t] == 1\n",
    "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
    "                                  legal_labels, gold_t))\n",
    "                if gold_t == self.n_trans - 1:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                elif gold_t < self.n_deprel: #\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                succ += 1\n",
    "                all_instances += instances\n",
    "        return all_instances\n",
    "\n",
    "    \n",
    "    def legal_labels(self, stack, buf):\n",
    "        \"\"\n",
    "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
    "        labels += [1] if len(buf) > 0 else [0]\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        \"\"\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        for i, example in enumerate(dataset):\n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]\n",
    "            sentences.append(sentence)\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "\n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "\n",
    "        UAS = all_tokens = 0.0\n",
    "        for i, ex in enumerate(dataset):\n",
    "            head = [-1] * len(ex['word'])\n",
    "            for h, t, in dependencies[i]:\n",
    "                head[t] = h\n",
    "            for pred_h, gold_h, gold_l, pos in \\\n",
    "                    zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
    "                    assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                    pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                    if (self.with_punct) or (not punct(self.language, pos_str)):\n",
    "                        UAS += 1 if pred_h == gold_h else 0\n",
    "                        all_tokens += 1\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserConfig(object):\n",
    "    language = 'english'\n",
    "    with_punct = True\n",
    "    unlabeled = True\n",
    "    lowercase = True\n",
    "    use_pos = True\n",
    "    use_dep = True\n",
    "    use_dep = use_dep and (not unlabeled)\n",
    "    data_path = os.path.join('assignment2', 'data')\n",
    "    train_file = 'train.conll'\n",
    "    dev_file = 'dev.conll'\n",
    "    test_file = 'test.conll'\n",
    "    embedding_file = 'en-cw.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"Initializes this partial parse.\"\n",
    "        self.sentence = sentence\n",
    "        self.stack = ['ROOT']\n",
    "        self.buffer = sentence.copy()\n",
    "        self.dependencies = []\n",
    "        \n",
    "        \n",
    "    def parse(self, transitions):\n",
    "        \"Applies the provided transitions\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "    \n",
    "    \n",
    "    def parse_step(self, transition):\n",
    "        \"Performs a single parse step with given transition\"\n",
    "        if transition == 'S':\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        elif transition == 'LA':\n",
    "            dependent = self.stack.pop(-2)\n",
    "            head = self.stack[-1]\n",
    "            self.dependencies.append((head, dependent))\n",
    "        else:\n",
    "            dependent = self.stack.pop()\n",
    "            head = self.stack[-1]\n",
    "            self.dependencies.append((head, dependent))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to generate minibatches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    \"\"\" \n",
    "    (1) data: there are two possible values:\n",
    "            - a list or numpy array\n",
    "            - a list where each element is either a list or numpy array\n",
    "    (2) minibatch_size: the maximum number of items in a minibatch\n",
    "    (3) shuffle: whether to randomize the order of returned data\n",
    "    Returns:\n",
    "        minibatches: the return value depends on data:\n",
    "            - If data is a list/array it yields the next minibatch of data.\n",
    "            - If data a list of lists/arrays it returns the next minibatch of \n",
    "              each element in the list. This can be used to iterate through \n",
    "              multiple data sources (e.g., features and labels) at the same time.\"\"\"\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
    "    data_size = len(data[0]) if list_data else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else _minibatch(data, minibatch_indices)\n",
    "\n",
    "\n",
    "def _minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)\n",
    "\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"Parses a list of sentences in minibatches using a model.\"\n",
    "\n",
    "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
    "    # https://docs.python.org/3.5/library/copy.html (A shallow copy constructs a new \n",
    "    # compound object and then (to the extent possible) inserts references into it to\n",
    "    # the objects found in the original.) \n",
    "    unfinished_parses = partial_parses.copy()\n",
    "    \n",
    "    # while unfinished_parses is not empty\n",
    "    while unfinished_parses:\n",
    "        # Take first batch_size parses in unfinished_parses as a minibatch\n",
    "        minibatch = unfinished_parses[:batch_size]\n",
    "        # Use model to predict the next transition for each partial parse in minibatch\n",
    "        while minibatch:\n",
    "            transitions = model.predict(minibatch) # the vectorized portion to gain a speed up \n",
    "            # Perform parse step on each partial parse in minibatch with predicted transition\n",
    "            for i, transition in enumerate(transitions):\n",
    "                minibatch[i].parse_step(transition)\n",
    "            # Remove completed (empty buffer and stack of size 1) parses from unfinished parses\n",
    "            minibatch = [pp for pp in minibatch if len(pp.stack) > 1 or len(pp.buffer) > 0]\n",
    "        del unfinished_parses[:batch_size]\n",
    "    dependencies = [partial_parse.dependencies for partial_parse in partial_parses]\n",
    "    return dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build and Train the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Fill in the hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig(object):\n",
    "    \"Holds model hyperparams and data information\"\n",
    "    n_features = 36\n",
    "    n_classes = 3\n",
    "    dropout = \n",
    "    embed_size = 50\n",
    "    hidden_size = \n",
    "    batch_size = \n",
    "    n_epochs = 10\n",
    "    lr = \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Complete the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel:\n",
    "    \"\"\"\n",
    "    Implements a feedforward neural network with an embedding layer and single hidden layer.\n",
    "    This network will predict which transition should be applied to a given partial parse\n",
    "    configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building and will be fed\n",
    "        data during training.  Note that when \"None\" is in a placeholder's shape, it's flexible\n",
    "        (so we can use different batch sizes without rebuilding the model).\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of  shape (None, n_features), type tf.int32\n",
    "        labels_placeholder: Labels placeholder tensor of shape (None, n_classes), type tf.float32\n",
    "        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "            self.dropout_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        \n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=0):\n",
    "       \"\"\"Creates the feed_dict for the dependency parser.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "        Hint: When an argument is None, don't add it to the feed_dict.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "            dropout: The dropout rate.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "    \n",
    "    \n",
    "    def add_embedding(self):\n",
    "        \"\"\"Adds an embedding layer that maps from input tokens (integers) to vectors and then\n",
    "        concatenates those vectors:\n",
    "            - Creates a tf.Variable and initializes it with self.pretrained_embeddings.\n",
    "            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n",
    "              tensor of shape (None, n_features, embedding_size).\n",
    "            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n",
    "              (None, n_features * embedding_size).\n",
    "\n",
    "        Hint: You might find tf.nn.embedding_lookup useful.\n",
    "        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n",
    "            what -1 in a shape means.\n",
    "            https://www.tensorflow.org/api_docs/python/tf/reshape\n",
    "\n",
    "        Returns:\n",
    "            embeddings: tf.Tensor of shape (None, n_features*embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the 1-hidden-layer NN:\n",
    "            h = Relu(xW + b1)\n",
    "            h_drop = Dropout(h, dropout_rate)\n",
    "            pred = h_dropU + b2\n",
    "\n",
    "        Note that we are not applying a softmax to pred. The softmax will instead be done in\n",
    "        the add_loss_op function, which improves efficiency because we can use\n",
    "        tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "        Use the initializer from q2_initialization.py to initialize W and U (you can initialize b1\n",
    "        and b2 with zeros)\n",
    "\n",
    "        Hint: Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument.\n",
    "              Therefore the keep probability should be set to the value of\n",
    "              (1 - self.dropout_placeholder)\n",
    "\n",
    "        Returns:\n",
    "            pred: tf.Tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.add_embedding()\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "    \n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "        In this case we are using cross entropy loss.\n",
    "        The loss should be averaged over all examples in the current minibatch.\n",
    "\n",
    "        Hint: You can use tf.nn.softmax_cross_entropy_with_logits_v2 and tf.stop_gradient to \n",
    "                simplify your implementation. You might find tf.reduce_mean useful.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n",
    "                  network before the softmax layer.\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Use tf.train.AdamOptimizer for this model.\n",
    "        Use the learning rate from self.config.\n",
    "        Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "    \n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n",
    "                                     dropout=self.config.dropout)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def predict_on_batch(self, sess, inputs_batch):\n",
    "        \"Make predictions for the provided batch of data\"\n",
    "        feed = self.create_feed_dict(inputs_batch)\n",
    "        predictions = sess.run(self.pred, feed_dict=feed)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def run_epoch(self, sess, parser, train_examples, dev_set):\n",
    "        \"Training loop\"\n",
    "        n_minibatches = 1 + len(train_examples) / self.config.batch_size\n",
    "        prog = tf.keras.utils.Progbar(target=n_minibatches)\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)):\n",
    "            loss = self.train_on_batch(sess, train_x, train_y)\n",
    "            prog.update(i + 1, [(\"train loss\", loss)])\n",
    "        print (\"\\nEvaluating on dev set\")\n",
    "        dev_UAS, _ = parser.parse(dev_set)\n",
    "        print (\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "        return dev_UAS\n",
    "    \n",
    "\n",
    "    def fit(self, sess, saver, parser, train_examples, dev_set):\n",
    "        best_dev_UAS = 0\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            print (\"Epoch {:} out of {:}\".format(epoch + 1, self.config.n_epochs))\n",
    "            dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set)\n",
    "            if dev_UAS > best_dev_UAS:\n",
    "                best_dev_UAS = dev_UAS\n",
    "                if saver:\n",
    "                    print (\"New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\\n\")\n",
    "                    saver.save(sess, './assignment2/data/weights/parser.weights')\n",
    "\n",
    "            \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_prediction_op()\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)\n",
    "\n",
    "        \n",
    "    def __init__(self, config, pretrained_embeddings):\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.config = config\n",
    "        self.build()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Test your model and iterate if necessary. The goal is to obtain training loss < 0.2 and dev UAS >= 65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True   \n",
    "print (\"Initializing...\")\n",
    "config = TrainConfig()\n",
    "parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n",
    "if not os.path.exists('./assignment2/data/weights/'):\n",
    "    os.makedirs('./assignment2/data/weights/')\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    print (\"Building model...\")\n",
    "    start = time.time()\n",
    "    model = ParserModel(config, embeddings)\n",
    "    parser.model = model\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = None if debug else tf.train.Saver()\n",
    "    print (\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "graph.finalize()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    parser.session = session\n",
    "    session.run(init_op)\n",
    "    print (\"Training...\")\n",
    "    model.fit(session, saver, parser, train_examples, dev_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.) Run your tested model on full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after results above show train loss < 0.2 and dev UAS is >= 65\n",
    "\n",
    "debug = False   \n",
    "print (\"Initializing...\")\n",
    "config = TrainConfig()\n",
    "parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n",
    "if not os.path.exists('./assignment2/data/weights/'):\n",
    "    os.makedirs('./assignment2/data/weights/')\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    print (\"Building model...\")\n",
    "    start = time.time()\n",
    "    model = ParserModel(config, embeddings)\n",
    "    parser.model = model\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = None if debug else tf.train.Saver()\n",
    "    print (\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "graph.finalize()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    parser.session = session\n",
    "    session.run(init_op)\n",
    "    print (\"Training...\")\n",
    "    model.fit(session, saver, parser, train_examples, dev_set)\n",
    "    if not debug:\n",
    "        print (\"Testing...\")\n",
    "        print (\"Restoring the best model weights found on the dev set\")\n",
    "        saver.restore(session, './assignment2/data/weights/parser.weights')\n",
    "        print (\"Final evaluation on test set\")\n",
    "        UAS, dependencies = parser.parse(test_set)\n",
    "        print (\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "        print (\"Writing predictions\")\n",
    "        with open('q2_test.predicted.pkl', 'wb') as f:\n",
    "            pickle.dump(dependencies, f, -1)\n",
    "        print (\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
