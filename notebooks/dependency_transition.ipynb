{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition-Based Neural Dependency Parser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chen and Manning (2014) [A Fast and Accurate Dependency Parser using Neural Networks](https://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf)\n",
    "* CS224n Assignment #2 (Winter 2018) [Question 2](http://web.stanford.edu/class/cs224n/assignment2/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tempfile import gettempdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for downloading and processing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penn Treebank-3 (https://catalog.ldc.upenn.edu/ldc99t42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, filename, expected_bytes):\n",
    "    \"Download the file if not present, make sure it's the right size.\"    \n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n",
    "        statinfo = os.stat(local_filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            with zipfile.ZipFile(local_filename) as f:\n",
    "                f.extractall()\n",
    "        else:\n",
    "            print(statinfo.st_size)\n",
    "            raise Exception('Failed to verify ' + local_filename + \n",
    "                            '. Can you get to it with a browser?')\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(in_file, lowercase=False, max_example=None):\n",
    "    \"\"\" Returns a list of examples where each example is a dict of lists:\n",
    "        'word' : list of str repr. the words in the sentence\n",
    "        'pos'  : list of str repr. the XPOS tags (language specific)\n",
    "        'head' : list of int repr. the position of each word's head word\n",
    "        'label': list of str repr. the dependency label of each word\"\"\"\n",
    "    examples = []\n",
    "    with open(in_file) as f:\n",
    "        word, pos, head, label = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            sp = line.strip().split('\\t')\n",
    "            if len(sp) == 10:\n",
    "                if '-' not in sp[0]:\n",
    "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
    "                    pos.append(sp[4])\n",
    "                    head.append(int(sp[6]))\n",
    "                    label.append(sp[7])\n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'label': label})\n",
    "                word, pos, head, label = [], [], [], []\n",
    "                if (max_example is not None) and (len(examples) == max_example):\n",
    "                    break\n",
    "        if len(word) > 0:\n",
    "            examples.append({'word': word, 'pos': pos,\n",
    "                             'head': head, 'label': label})\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(keys, n_max=None, offset=0):\n",
    "    \"Helper function for building the mapping dicts\"\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    ls = count.most_common() if n_max is None \\\n",
    "        else count.most_common(n_max)\n",
    "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct(language, pos):\n",
    "    if language == 'english':\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    elif language == 'universal':\n",
    "        return pos == 'PUNCT'\n",
    "    else:\n",
    "        raise ValueError('language: %s is not supported.' % language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(reduced=True):\n",
    "    config = ParserConfig()\n",
    "    print(\"Downloading data...\")\n",
    "    start = time.time()\n",
    "    zipfile = maybe_download('http://web.stanford.edu/class/cs224n/assignment2/',\n",
    "                             'assignment2.zip',38866004)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "    \n",
    "    print (\"Loading data...\"),\n",
    "    start = time.time()\n",
    "    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n",
    "                           lowercase=config.lowercase)\n",
    "    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n",
    "                         lowercase=config.lowercase)\n",
    "    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n",
    "                          lowercase=config.lowercase)\n",
    "    if reduced:\n",
    "        train_set = train_set[:1000]\n",
    "        dev_set = dev_set[:500]\n",
    "        test_set = test_set[:500]\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Building parser...\")\n",
    "    start = time.time()\n",
    "    parser = Parser(train_set)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Loading pretrained embeddings...\")\n",
    "    start = time.time()\n",
    "    word_vectors = {}\n",
    "    for line in open(os.path.join(config.data_path, config.embedding_file)):\n",
    "        sp = line.strip().split()\n",
    "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
    "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "    for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Vectorizing data...\")\n",
    "    start = time.time()\n",
    "    train_set = parser.vectorize(train_set)\n",
    "    dev_set = parser.vectorize(dev_set)\n",
    "    test_set = parser.vectorize(test_set)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    print (\"Preprocessing training data...\")\n",
    "    start = time.time()\n",
    "    train_examples = parser.create_instances(train_set)\n",
    "    print (\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "    return parser, embeddings_matrix, train_examples, dev_set, test_set,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for Dependency Parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "        pred = self.parser.model.predict_on_batch(self.parser.session, mb_x)\n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_PREFIX = '<p>:'\n",
    "L_PREFIX = '<l>:'\n",
    "UNK = '<UNK>'\n",
    "NULL = '<NULL>'\n",
    "ROOT = '<ROOT>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    \"Contains everything needed for transition-based dependency parsing except for the model\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        # Check that there is a unique label for root\n",
    "        root_labels = list([l for ex in dataset\n",
    "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
    "        counter = Counter(root_labels)\n",
    "        if len(counter) > 1:\n",
    "            print('Warning: more than one root label')\n",
    "            print(counter)\n",
    "        self.root_label = counter.most_common()[0][0]\n",
    "        \n",
    "        # list of all unique dependency labels\n",
    "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
    "                                               for w in ex['label']\n",
    "                                               if w != self.root_label]))\n",
    "        \n",
    "        # DEP labels such as <l>:acl, ... , <l>:xcomp, <l>:<NULL>\n",
    "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
    "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
    "\n",
    "        config = ParserConfig()\n",
    "        self.unlabeled = config.unlabeled\n",
    "        self.with_punct = config.with_punct\n",
    "        self.use_pos = config.use_pos\n",
    "        self.use_dep = config.use_dep\n",
    "        self.language = config.language\n",
    "        \n",
    "        # dictionaries for transitions (left and right) incl. S for shift\n",
    "        if self.unlabeled:\n",
    "            trans = ['L', 'R', 'S']\n",
    "            self.n_deprel = 1\n",
    "        else: \n",
    "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
    "            self.n_deprel = len(deprel)\n",
    "\n",
    "        self.n_trans = len(trans) # number of unique arc-transitions\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "        \n",
    "        # POS tags such as <p>:$, ... , <p>:<UNK>, <p>:<NULL>, <p>:<ROOT>\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "        \n",
    "        # Words including <UNK>, <NULL>, <ROOT>\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK] = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "\n",
    "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
    "        self.n_tokens = len(tok2id)\n",
    "\n",
    "    def vectorize(self, examples):\n",
    "        \" Numericalizes examples and adds ROOT to the front\"\n",
    "        vec_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['label']]\n",
    "            vec_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'label': label})\n",
    "        return vec_examples\n",
    "\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "        \"Returns list of features described in sec 3.1 of Chen and Manning\"\n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0\n",
    "\n",
    "        def get_lc(k):\n",
    "            # sorted indices of all left children of word at position k \n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        def get_rc(k):\n",
    "            # sorted indices of all right children of word at position k\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        p_features = []\n",
    "        l_features = []\n",
    "        # top 3 words on the stack (s1, s2, s3)\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        # top 3 words in the buffer\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        if self.use_pos: # add POS tags for top 3 words on stack and in buffer\n",
    "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "\n",
    "        # first and second leftmost/rightmost children of top 2 words on stack\n",
    "        for i in range(2):\n",
    "            if i < len(stack):   # check that there are sufficient words on stack\n",
    "                k = stack[-i-1]  # index into top 2 words on stack\n",
    "                lc = get_lc(k)   # all left children\n",
    "                rc = get_rc(k)   # all right children\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else [] # left children of leftmost child\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else [] # right children of rightmost child\n",
    "                # first and second leftmost/rightmost children\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                # leftmost child of leftmost child\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                # rightmost child of rightmost child\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                if self.use_pos:\n",
    "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "\n",
    "                if self.use_dep:\n",
    "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
    "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
    "            else:\n",
    "                features += [self.NULL] * 6\n",
    "                if self.use_pos:\n",
    "                    p_features += [self.P_NULL] * 6\n",
    "                if self.use_dep:\n",
    "                    l_features += [self.L_NULL] * 6\n",
    "\n",
    "        features += p_features + l_features\n",
    "        assert len(features) == self.n_features\n",
    "        return features\n",
    "    \n",
    "\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        \"\"\" Implements 'shortest stack oracle' in sec 3.2 of Chen and Manning\n",
    "            Returns:\n",
    "            (1) 2 (if unlabeled) or index for S (shift operation)\n",
    "            (2) 0 (if unlabeled) or DEP label of second word on stack, s2 (left-arc)\n",
    "            (3) 1 (if unlabeled) or DEP label of first word on stack, s1 (right arc)\n",
    "        \"\"\"\n",
    "        if len(stack) < 2:            # stack contans only one word\n",
    "            return self.n_trans - 1   # return shift operation\n",
    "\n",
    "        i0 = stack[-1]                # position of top two words on stack \n",
    "        i1 = stack[-2]\n",
    "        h0 = ex['head'][i0]           # position of their respective heads\n",
    "        h1 = ex['head'][i1]\n",
    "        l0 = ex['label'][i0]          # their respective dependency labels\n",
    "        l1 = ex['label'][i1]\n",
    "\n",
    "        if self.unlabeled:\n",
    "            if (i1 > 0) and (h1 == i0):  # second on stack is not root  \n",
    "                return 0                 # second on stack is dep on first\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return 1\n",
    "            else:\n",
    "                return None if len(buf) == 0 else 2\n",
    "        else:\n",
    "            if (i1 > 0) and (h1 == i0):\n",
    "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
    "            elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
    "            else:\n",
    "                return None if len(buf) == 0 else self.n_trans - 1\n",
    "\n",
    "            \n",
    "    def create_instances(self, examples):\n",
    "        \"\"\n",
    "        all_instances = []\n",
    "        succ = 0\n",
    "        for id, ex in enumerate(examples):\n",
    "            n_words = len(ex['word']) - 1\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            for i in range(n_words * 2):\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                legal_labels = self.legal_labels(stack, buf)\n",
    "                assert legal_labels[gold_t] == 1\n",
    "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
    "                                  legal_labels, gold_t))\n",
    "                if gold_t == self.n_trans - 1:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                elif gold_t < self.n_deprel: #\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                succ += 1\n",
    "                all_instances += instances\n",
    "        return all_instances\n",
    "\n",
    "    \n",
    "    def legal_labels(self, stack, buf):\n",
    "        \"\"\n",
    "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
    "        labels += [1] if len(buf) > 0 else [0]\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        \"\"\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        for i, example in enumerate(dataset):\n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]\n",
    "            sentences.append(sentence)\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "\n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "\n",
    "        UAS = all_tokens = 0.0\n",
    "        for i, ex in enumerate(dataset):\n",
    "            head = [-1] * len(ex['word'])\n",
    "            for h, t, in dependencies[i]:\n",
    "                head[t] = h\n",
    "            for pred_h, gold_h, gold_l, pos in \\\n",
    "                    zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
    "                    assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                    pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                    if (self.with_punct) or (not punct(self.language, pos_str)):\n",
    "                        UAS += 1 if pred_h == gold_h else 0\n",
    "                        all_tokens += 1\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserConfig(object):\n",
    "    language = 'english'\n",
    "    with_punct = True\n",
    "    unlabeled = True\n",
    "    lowercase = True\n",
    "    use_pos = True\n",
    "    use_dep = True\n",
    "    use_dep = use_dep and (not unlabeled)\n",
    "    data_path = os.path.join('assignment2', 'data')\n",
    "    train_file = 'train.conll'\n",
    "    dev_file = 'dev.conll'\n",
    "    test_file = 'test.conll'\n",
    "    embedding_file = 'en-cw.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"Initializes this partial parse.\"\n",
    "        self.sentence = sentence\n",
    "        self.stack = ['ROOT']\n",
    "        self.buffer = sentence.copy()\n",
    "        self.dependencies = []\n",
    "        \n",
    "        \n",
    "    def parse(self, transitions):\n",
    "        \"Applies the provided transitions\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "    \n",
    "    \n",
    "    def parse_step(self, transition):\n",
    "        \"Performs a single parse step with given transition\"\n",
    "        if transition == 'S':\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        elif transition == 'LA':\n",
    "            dependent = self.stack.pop(-2)\n",
    "            head = self.stack[-1]\n",
    "            self.dependencies.append((head, dependent))\n",
    "        else:\n",
    "            dependent = self.stack.pop()\n",
    "            head = self.stack[-1]\n",
    "            self.dependencies.append((head, dependent))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to generate minibatches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    \"\"\" \n",
    "    (1) data: there are two possible values:\n",
    "            - a list or numpy array\n",
    "            - a list where each element is either a list or numpy array\n",
    "    (2) minibatch_size: the maximum number of items in a minibatch\n",
    "    (3) shuffle: whether to randomize the order of returned data\n",
    "    Returns:\n",
    "        minibatches: the return value depends on data:\n",
    "            - If data is a list/array it yields the next minibatch of data.\n",
    "            - If data a list of lists/arrays it returns the next minibatch of \n",
    "              each element in the list. This can be used to iterate through \n",
    "              multiple data sources (e.g., features and labels) at the same time.\"\"\"\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
    "    data_size = len(data[0]) if list_data else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else _minibatch(data, minibatch_indices)\n",
    "\n",
    "\n",
    "def _minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)\n",
    "\n",
    "\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"Parses a list of sentences in minibatches using a model.\"\n",
    "\n",
    "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
    "    # https://docs.python.org/3.5/library/copy.html (A shallow copy constructs a new \n",
    "    # compound object and then (to the extent possible) inserts references into it to\n",
    "    # the objects found in the original.) \n",
    "    unfinished_parses = partial_parses.copy()\n",
    "    \n",
    "    # while unfinished_parses is not empty\n",
    "    while unfinished_parses:\n",
    "        # Take first batch_size parses in unfinished_parses as a minibatch\n",
    "        minibatch = unfinished_parses[:batch_size]\n",
    "        # Use model to predict the next transition for each partial parse in minibatch\n",
    "        while minibatch:\n",
    "            transitions = model.predict(minibatch) # the vectorized portion to gain a speed up \n",
    "            # Perform parse step on each partial parse in minibatch with predicted transition\n",
    "            for i, transition in enumerate(transitions):\n",
    "                minibatch[i].parse_step(transition)\n",
    "            # Remove completed (empty buffer and stack of size 1) parses from unfinished parses\n",
    "            minibatch = [pp for pp in minibatch if len(pp.stack) > 1 or len(pp.buffer) > 0]\n",
    "        del unfinished_parses[:batch_size]\n",
    "    dependencies = [partial_parse.dependencies for partial_parse in partial_parses]\n",
    "    return dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Section 4.2 of Chen and Manning (2014) for some of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig(object):\n",
    "    \"Holds model hyperparams and data information\"\n",
    "    n_features = 36\n",
    "    n_classes = 3\n",
    "    dropout = 0.5\n",
    "    embed_size = 50\n",
    "    hidden_size = 200\n",
    "    batch_size = 1024\n",
    "    n_epochs = 10\n",
    "    lr = 0.0005          # paper uses 0.01 with Adagrad\n",
    "    l2_reg = 10e-8 \n",
    "    activation = 'cube'  # relu gives similar results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel:\n",
    "    \"\"\"\n",
    "    Implements a feedforward neural network with an embedding layer and single hidden layer.\n",
    "    This network will predict which transition should be applied to a given partial parse\n",
    "    configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.input_placeholder = tf.placeholder(tf.int32, shape=(None, self.config.n_features))\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, shape=(None, self.config.n_classes))\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=0):\n",
    "        \"Creates the feed_dict for the dependency parser.\"\n",
    "        feed_dict = {self.input_placeholder: inputs_batch,\n",
    "                     self.dropout_placeholder: dropout}\n",
    "        if labels_batch is not None:\n",
    "            feed_dict.update({self.labels_placeholder: labels_batch})\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "        self.embedding_weights = tf.get_variable('embedding_w', dtype=tf.float32,\n",
    "                                                 initializer=self.pretrained_embeddings)\n",
    "        embeddings = tf.nn.embedding_lookup(self.embedding_weights, self.input_placeholder)\n",
    "        embeddings = tf.reshape(embeddings,\n",
    "                                shape=(-1, self.config.n_features*self.config.embed_size))        \n",
    "        return embeddings\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        x = self.add_embedding()\n",
    "        h = tf.layers.dense(x, self.config.hidden_size, \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        h = tf.pow(h, 3) if self.config.activation == 'cube' else tf.nn.relu(h)\n",
    "        h_drop = tf.nn.dropout(h, 1 - self.dropout_placeholder)\n",
    "        pred = tf.layers.dense(h_drop, self.config.n_classes, use_bias=False,\n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        return pred\n",
    "\n",
    "    \n",
    "    def add_loss_op(self, pred):\n",
    "        \"Adds op loss function to the computational graph.\"\n",
    "        labels = tf.stop_gradient(self.labels_placeholder)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=labels, logits=pred)) + self.config.l2_reg\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"Sets up the training op\"\n",
    "        train_op = tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n",
    "        return train_op\n",
    "    \n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n",
    "                                     dropout=self.config.dropout)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def predict_on_batch(self, sess, inputs_batch):\n",
    "        \"Make predictions for the provided batch of data\"\n",
    "        feed = self.create_feed_dict(inputs_batch)\n",
    "        predictions = sess.run(self.pred, feed_dict=feed)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def run_epoch(self, sess, parser, train_examples, dev_set):\n",
    "        \"Training loop\"\n",
    "        n_minibatches = 1 + len(train_examples) / self.config.batch_size\n",
    "        prog = tf.keras.utils.Progbar(target=n_minibatches)\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)):\n",
    "            loss = self.train_on_batch(sess, train_x, train_y)\n",
    "            prog.update(i + 1, [(\"train loss\", loss)])\n",
    "        print (\"\\nEvaluating on dev set\")\n",
    "        dev_UAS, _ = parser.parse(dev_set)\n",
    "        print (\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "        return dev_UAS\n",
    "    \n",
    "\n",
    "    def fit(self, sess, saver, parser, train_examples, dev_set):\n",
    "        best_dev_UAS = 0\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            print (\"Epoch {:} out of {:}\".format(epoch + 1, self.config.n_epochs))\n",
    "            dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set)\n",
    "            if dev_UAS > best_dev_UAS:\n",
    "                best_dev_UAS = dev_UAS\n",
    "                if saver:\n",
    "                    print (\"New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\\n\")\n",
    "                    saver.save(sess, './assignment2/data/weights/parser.weights')\n",
    "\n",
    "            \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_prediction_op()\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)\n",
    "\n",
    "        \n",
    "    def __init__(self, config, pretrained_embeddings):\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.config = config\n",
    "        self.build()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Downloading data...\n",
      "took 113.06 seconds\n",
      "Loading data...\n",
      "took 1.70 seconds\n",
      "Building parser...\n",
      "took 1.00 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 2.11 seconds\n",
      "Vectorizing data...\n",
      "took 1.32 seconds\n",
      "Preprocessing training data...\n",
      "took 40.89 seconds\n",
      "Building model...\n",
      "took 2.26 seconds\n",
      "\n",
      "Training...\n",
      "Epoch 1 out of 10\n",
      "1845/1848 [============================>.] - ETA: 0s - train loss: 0.2840\n",
      "Evaluating on dev set\n",
      "- dev UAS: 81.51\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Epoch 2 out of 10\n",
      "1841/1848 [============================>.] - ETA: 0s - train loss: 0.1423\n",
      "Evaluating on dev set\n",
      "- dev UAS: 84.04\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Epoch 3 out of 10\n",
      "1846/1848 [============================>.] - ETA: 0s - train loss: 0.1243\n",
      "Evaluating on dev set\n",
      "- dev UAS: 85.34\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Epoch 4 out of 10\n",
      "1847/1848 [============================>.] - ETA: 0s - train loss: 0.1142\n",
      "Evaluating on dev set\n",
      "- dev UAS: 86.36\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Epoch 5 out of 10\n",
      "1843/1848 [============================>.] - ETA: 0s - train loss: 0.1071\n",
      "Evaluating on dev set\n",
      "- dev UAS: 86.34\n",
      "Epoch 6 out of 10\n",
      "1843/1848 [============================>.] - ETA: 0s - train loss: 0.1016\n",
      "Evaluating on dev set\n",
      "- dev UAS: 87.17\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Epoch 7 out of 10\n",
      "1843/1848 [============================>.] - ETA: 0s - train loss: 0.0971\n",
      "Evaluating on dev set\n",
      "- dev UAS: 86.92\n",
      "Epoch 8 out of 10\n",
      "1842/1848 [============================>.] - ETA: 0s - train loss: 0.0930\n",
      "Evaluating on dev set\n",
      "- dev UAS: 87.36\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Epoch 9 out of 10\n",
      "1845/1848 [============================>.] - ETA: 0s - train loss: 0.0895\n",
      "Evaluating on dev set\n",
      "- dev UAS: 87.42\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Epoch 10 out of 10\n",
      "1847/1848 [============================>.] - ETA: 0s - train loss: 0.0867\n",
      "Evaluating on dev set\n",
      "- dev UAS: 87.82\n",
      "New best dev UAS! Saving model in ./assignment2/data/weights/parser.weights\n",
      "\n",
      "Testing...\n",
      "Restoring the best model weights found on the dev set\n",
      "INFO:tensorflow:Restoring parameters from ./assignment2/data/weights/parser.weights\n",
      "Final evaluation on test set\n",
      "- test UAS: 88.06\n",
      "Writing predictions\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "print (\"Initializing...\")\n",
    "config = TrainConfig()\n",
    "parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n",
    "if not os.path.exists('./assignment2/data/weights/'):\n",
    "    os.makedirs('./assignment2/data/weights/')\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    print (\"Building model...\")\n",
    "    start = time.time()\n",
    "    model = ParserModel(config, embeddings)\n",
    "    parser.model = model\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = None if debug else tf.train.Saver()\n",
    "    print (\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "graph.finalize()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    parser.session = session\n",
    "    session.run(init_op)\n",
    "    print (\"Training...\")\n",
    "    model.fit(session, saver, parser, train_examples, dev_set)\n",
    "    if not debug:\n",
    "        print (\"Testing...\")\n",
    "        print (\"Restoring the best model weights found on the dev set\")\n",
    "        saver.restore(session, './assignment2/data/weights/parser.weights')\n",
    "        print (\"Final evaluation on test set\")\n",
    "        UAS, dependencies = parser.parse(test_set)\n",
    "        print (\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "        print (\"Writing predictions\")\n",
    "        with open('q2_test.predicted.pkl', 'wb') as f:\n",
    "            pickle.dump(dependencies, f, -1)\n",
    "        print (\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
