{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* Stanford CS224n Lecture 4 (Winter 2018) [Slides](https://web.stanford.edu/class/cs224n/lectures/lecture4.pdf)\n",
    "* Stanford CS224n Lecture 4 (Winter 2017) [Video](https://youtu.be/uc2_iwVqrRI)\n",
    "* Denny Britz's post (2015): [Implementing a Neural Network from Scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "from tempfile import gettempdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "print('Python', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Classify whether the center word within a window of words is a location\n",
    "\n",
    "* To build a simple neural network model to illustrate non-linear function approximation, backpropagation and stochastic gradient descent.\n",
    "* Background to the Named Entity Recognition (NER) problem on [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "* Description of task in Stanford CS224n Lecture 4 [slide 45](https://web.stanford.edu/class/cs224n/lectures/lecture4.pdf#page=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data from file\n",
    "Tjong Kim Sang et al. [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition\n",
    "](http://www.aclweb.org/anthology/W03-0419.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, filename, expected_bytes):\n",
    "    \"Download the file if not present, and make sure it's the right size.\"    \n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n",
    "        statinfo = os.stat(local_filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print('Found and verified', filename)\n",
    "        else:\n",
    "            print(statinfo.st_size)\n",
    "            raise Exception('Failed to verify ' + local_filename + \n",
    "                            '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"Reads the eng.train data file from CONLL2003\"\n",
    "    sents, sent_tags = [], []\n",
    "    with open(filename) as f:\n",
    "        dictionary = {'<PAD>': 0}\n",
    "        sent, tags = [], []\n",
    "        for line in f:\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                continue\n",
    "            if line.startswith('\\n'):\n",
    "                if sent and tags:\n",
    "                    sents.append(sent)\n",
    "                    sent_tags.append(tags)\n",
    "                    sent, tags = [], []\n",
    "                continue\n",
    "            word, _, _, tag = line.split()\n",
    "            sent.append(word)\n",
    "            tags.append(tag)\n",
    "            if not dictionary.get(word):\n",
    "                dictionary[word] = len(dictionary)\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return sents, sent_tags, dictionary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = maybe_download(\n",
    "    url='https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/',\n",
    "    filename='eng.train',\n",
    "    expected_bytes=3283420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to view raw text: https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from file and build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sents               : list of sentences (where each sentence is a list of words)\n",
    "    sent_tags           : list of named-entity tags corresponding to each word in sents\n",
    "    dictionary          : maps words(strings) to their IDs(int)\n",
    "    reversed_dictionary : maps IDs(int) to their words(strings)\n",
    "\"\"\"\n",
    "sents, sent_tags, dictionary, reversed_dictionary = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'] ['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('Sample sentence:', sents[0], sent_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23624"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)   # vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word windows for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_windows(window_size=2):\n",
    "    \"\"\"\n",
    "    Param: window_size (int) for each side of center word\n",
    "    Returns: : tuple (list of +ve windows, list of -ve windows)\n",
    "    \"\"\"\n",
    "    pos_windows, neg_windows = [], []\n",
    "    span = 2*window_size + 1\n",
    "    for sent, tags in zip(sents, sent_tags):\n",
    "        count = len(sent)\n",
    "        # pad sentence at front and end\n",
    "        sent = [0]*window_size + [dictionary[word] for word in sent] + [0]*window_size\n",
    "        for i in range(count):\n",
    "            window = sent[i:i+span]\n",
    "            # positive if center word is tagged as location\n",
    "            if tags[i] in ['B-LOC', 'I-LOC']:\n",
    "                pos_windows.append(window)\n",
    "            else:\n",
    "                neg_windows.append(window)\n",
    "    return pos_windows, neg_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive windows:  8297\n",
      "Number of negative windows:  195324\n",
      "Sample positive window:  [0, 0, 12, 13, 0] ['<PAD>', '<PAD>', 'BRUSSELS', '1996-08-22', '<PAD>']\n",
      "Sample negative window:  [0, 0, 1, 2, 3] ['<PAD>', '<PAD>', 'EU', 'rejects', 'German']\n"
     ]
    }
   ],
   "source": [
    "pos_windows, neg_windows = prepare_windows(window_size=2)\n",
    "print('Number of positive windows: ', len(pos_windows))\n",
    "print('Number of negative windows: ', len(neg_windows))\n",
    "print('Sample positive window: ', pos_windows[0], [reversed_dictionary[i] for i in pos_windows[0]])\n",
    "print('Sample negative window: ', neg_windows[0], [reversed_dictionary[i] for i in neg_windows[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameter values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "embedding_size = 128      # word embdedding dimension size \n",
    "window_size = 2           # size of window on each side of center word\n",
    "hidden_size = 256         # size of the hidden layer\n",
    "learning_rate = 0.02     # initial learning rate\n",
    "num_epochs = 100         # number of passes over true window samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test values to monitor training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent = [dictionary[word] for word in 'shops in Paris are amazing'.split()]\n",
    "neg_sent = [dictionary[word] for word in 'not all shops in Paris'.split()]\n",
    "\n",
    "word_ids = np.vstack((pos_sent, neg_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive window (\"shops in Paris are amazing\") score: -0.49831538585163093\n",
      "Negative window (\"not all shops in Paris\") score: -0.7403504434077313\n",
      "Epoch 1 error:  0.7391885316521911\n",
      "Epoch 2 error:  0.6161211939269448\n",
      "Epoch 3 error:  0.4799475136994054\n",
      "Epoch 4 error:  0.4056838520055447\n",
      "Epoch 5 error:  0.33126645288134854\n",
      "Epoch 6 error:  0.26422693748680515\n",
      "Epoch 7 error:  0.2503251895116747\n",
      "Epoch 8 error:  0.22032977100542472\n",
      "Epoch 9 error:  0.19144447408107104\n",
      "Positive window (\"shops in Paris are amazing\") score: 5.5460618322367985\n",
      "Negative window (\"not all shops in Paris\") score: -4.463953210515129\n",
      "Epoch 10 error:  0.17088307133201136\n",
      "Epoch 11 error:  0.1536539141041873\n",
      "Epoch 12 error:  0.14803491252871265\n",
      "Epoch 13 error:  0.13151828208770408\n",
      "Epoch 14 error:  0.1119861441775833\n",
      "Epoch 15 error:  0.1125535749307099\n",
      "Epoch 16 error:  0.1061423393452722\n",
      "Epoch 17 error:  0.09046600177623859\n",
      "Epoch 18 error:  0.08860304907480908\n",
      "Epoch 19 error:  0.0799735343874776\n",
      "Positive window (\"shops in Paris are amazing\") score: -1.384110329808645\n",
      "Negative window (\"not all shops in Paris\") score: -13.00355419830204\n",
      "Epoch 20 error:  0.08152304455152641\n",
      "Epoch 21 error:  0.07438225424478605\n",
      "Epoch 22 error:  0.07726100695201238\n",
      "Epoch 23 error:  0.06639971227843197\n",
      "Epoch 24 error:  0.060519147054834244\n",
      "Epoch 25 error:  0.06252026684571382\n",
      "Epoch 26 error:  0.06086574828889513\n",
      "Epoch 27 error:  0.06512980332966042\n",
      "Epoch 28 error:  0.05681985349774494\n",
      "Epoch 29 error:  0.04362462771413112\n",
      "Positive window (\"shops in Paris are amazing\") score: -4.3489836249366824\n",
      "Negative window (\"not all shops in Paris\") score: -15.608057055113038\n",
      "Epoch 30 error:  0.056644218362329184\n",
      "Epoch 31 error:  0.05122652642926275\n",
      "Epoch 32 error:  0.05321724283035415\n",
      "Epoch 33 error:  0.047698391180630406\n",
      "Epoch 34 error:  0.04585219349097687\n",
      "Epoch 35 error:  0.04086253529590112\n",
      "Epoch 36 error:  0.04151864543988138\n",
      "Epoch 37 error:  0.039434971202357894\n",
      "Epoch 38 error:  0.042104668567399896\n",
      "Epoch 39 error:  0.04192425844800129\n",
      "Positive window (\"shops in Paris are amazing\") score: -3.168022929767007\n",
      "Negative window (\"not all shops in Paris\") score: -19.87545180151988\n",
      "Epoch 40 error:  0.03737286264073497\n",
      "Epoch 41 error:  0.037711032866518\n",
      "Epoch 42 error:  0.041458163865174334\n",
      "Epoch 43 error:  0.03454382670922698\n",
      "Epoch 44 error:  0.02798705288884366\n",
      "Epoch 45 error:  0.033562408193865335\n",
      "Epoch 46 error:  0.02772658391675709\n",
      "Epoch 47 error:  0.02710457811573508\n",
      "Epoch 48 error:  0.02981828223926092\n",
      "Epoch 49 error:  0.02708638475254989\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.4359027713641488\n",
      "Negative window (\"not all shops in Paris\") score: -14.579881976346384\n",
      "Epoch 50 error:  0.028512729645847962\n",
      "Epoch 51 error:  0.03546176300080792\n",
      "Epoch 52 error:  0.03105456730990114\n",
      "Epoch 53 error:  0.025316388632244974\n",
      "Epoch 54 error:  0.02463476831938361\n",
      "Epoch 55 error:  0.019183604673341993\n",
      "Epoch 56 error:  0.023331592026834266\n",
      "Epoch 57 error:  0.022087296516386436\n",
      "Epoch 58 error:  0.023422094523051475\n",
      "Epoch 59 error:  0.017462541664000037\n",
      "Positive window (\"shops in Paris are amazing\") score: 2.27439591538967\n",
      "Negative window (\"not all shops in Paris\") score: -12.71690540185843\n",
      "Epoch 60 error:  0.018484413352654157\n",
      "Epoch 61 error:  0.02077681478416215\n",
      "Epoch 62 error:  0.027587987334020906\n",
      "Epoch 63 error:  0.017067036735290888\n",
      "Epoch 64 error:  0.019381894962088392\n",
      "Epoch 65 error:  0.01977057030736228\n",
      "Epoch 66 error:  0.020456221541603384\n",
      "Epoch 67 error:  0.020650260714942564\n",
      "Epoch 68 error:  0.021101317488484\n",
      "Epoch 69 error:  0.019822850553395516\n",
      "Positive window (\"shops in Paris are amazing\") score: 5.575982818592403\n",
      "Negative window (\"not all shops in Paris\") score: -9.483883308994812\n",
      "Epoch 70 error:  0.01689035282348148\n",
      "Epoch 71 error:  0.01600141475904577\n",
      "Epoch 72 error:  0.02201159869025234\n",
      "Epoch 73 error:  0.017459526007112713\n",
      "Epoch 74 error:  0.01794566493426897\n",
      "Epoch 75 error:  0.01626091394491161\n",
      "Epoch 76 error:  0.018147503974507092\n",
      "Epoch 77 error:  0.016658117413283907\n",
      "Epoch 78 error:  0.017624472435729763\n",
      "Epoch 79 error:  0.016566840234694805\n",
      "Positive window (\"shops in Paris are amazing\") score: 6.679279038501981\n",
      "Negative window (\"not all shops in Paris\") score: -10.433244925146981\n",
      "Epoch 80 error:  0.018431262703492445\n",
      "Epoch 81 error:  0.02049059679488383\n",
      "Epoch 82 error:  0.020112822033574194\n",
      "Epoch 83 error:  0.029448193686745092\n",
      "Epoch 84 error:  0.0173903638401228\n",
      "Epoch 85 error:  0.020919733590235835\n",
      "Epoch 86 error:  0.017356800742514233\n",
      "Epoch 87 error:  0.01861966761572127\n",
      "Epoch 88 error:  0.0181222889874606\n",
      "Epoch 89 error:  0.014475624285917303\n",
      "Positive window (\"shops in Paris are amazing\") score: 11.589415010745189\n",
      "Negative window (\"not all shops in Paris\") score: -9.520891494369423\n",
      "Epoch 90 error:  0.016083166344656494\n",
      "Epoch 91 error:  0.01391977133907784\n",
      "Epoch 92 error:  0.0139124854336207\n",
      "Epoch 93 error:  0.015648673076892338\n",
      "Epoch 94 error:  0.018501124163715757\n",
      "Epoch 95 error:  0.016829732576503068\n",
      "Epoch 96 error:  0.016243496252158884\n",
      "Epoch 97 error:  0.013408082716114943\n",
      "Epoch 98 error:  0.014214045043233819\n",
      "Epoch 99 error:  0.01691725161941999\n",
      "Positive window (\"shops in Paris are amazing\") score: 13.438604562413705\n",
      "Negative window (\"not all shops in Paris\") score: -7.229528302753981\n",
      "Epoch 100 error:  0.014867091107250211\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This code trains a simple neural network as a binary classifier. \n",
    "    The model calculates a score when it is given a window of words. \n",
    "    The score is used to determine whether the center word in the\n",
    "    window is a location or not.\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(seed)\n",
    "vocab_size = len(dictionary)\n",
    "x_dim = embedding_size * (2*window_size + 1)\n",
    "\n",
    "# Initialize model parameters \n",
    "embeddings = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_size))\n",
    "W = np.random.randn(x_dim, hidden_size) * np.sqrt(1.0/x_dim)\n",
    "b = np.zeros(hidden_size)\n",
    "u = np.random.randn(hidden_size)\n",
    "\n",
    "average_error = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, pos_window in enumerate(pos_windows):\n",
    "        neg_window = random.sample(neg_windows, k=1) # s_c\n",
    "        inputs = np.vstack((pos_window, neg_window)) # stack as matrix\n",
    "        X = embeddings[inputs].reshape(-1, x_dim)    # concat the words\n",
    "        \n",
    "        # Forward pass    \n",
    "        z = X.dot(W) + b                # affine transformation\n",
    "        a = 1. / (1. + np.exp(-z))      # non-linearity (sigmoid)\n",
    "        scores = a.dot(u)               # scalar unnormalized scores\n",
    "\n",
    "        # Max-margin objective\n",
    "        error = 1 if max(0, 1 - scores[0] + scores[1]) > 0 else 0\n",
    "        \n",
    "        # Backward pass (no updating if error is 0)\n",
    "        grad_u = error * (a[1] - a[0])       # gradient for u\n",
    "        delta = grad_u * (a*(1 - a))         # multiply with sigmoid derivative\n",
    "        grad_W = X.T.dot(delta)              # gradient for W\n",
    "        grad_b = delta.sum(axis=0)           # gradient for b\n",
    "        grad_X = delta.dot(W.T)              # gradient for the word vectors\n",
    "        grad_X = grad_X.reshape(-1, 2*window_size + 1, embedding_size)                                \n",
    "        \n",
    "        # Parameter updates using gradient descent\n",
    "        u -= learning_rate * grad_u\n",
    "        W -= learning_rate * grad_W\n",
    "        b -= learning_rate * grad_b\n",
    "        embeddings[inputs] -= learning_rate * grad_X\n",
    "        \n",
    "        # Keep track of any errors\n",
    "        if error == 1: average_error += 1 - scores[0] + scores[1]\n",
    "         \n",
    "    # Check scores for test pair\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        X_test = embeddings[word_ids].reshape(-1, x_dim)\n",
    "        z_test = X_test.dot(W) + b\n",
    "        a_test = 1. / (1. + np.exp(-z_test))\n",
    "        scores_test = a_test.dot(u)\n",
    "        print('Positive window (\"shops in Paris are amazing\") score:', scores_test[0])\n",
    "        print('Negative window (\"not all shops in Paris\") score:', scores_test[1])\n",
    "\n",
    "    # Print average error per epoch\n",
    "    print('Epoch', epoch + 1, 'error: ', average_error / i)\n",
    "    \n",
    "    # Stop training when average error is low enough\n",
    "    if average_error / i < 0.01:\n",
    "        break\n",
    "    \n",
    "    average_error = 0\n",
    "        \n",
    "    # Decay learning rate exponentially every epoch\n",
    "    learning_rate = learning_rate * 0.9999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 20.5939542990684 Min: -5.918761318490564 Mean: 5.916601574099561 Median: 5.719148857009731\n"
     ]
    }
   ],
   "source": [
    "# score statistics for all the positive windows in the training set\n",
    "X_test = embeddings[pos_windows].reshape(-1, x_dim)\n",
    "z_test = X_test.dot(W) + b\n",
    "a_test = 1. / (1. + np.exp(-z_test))\n",
    "score_test = a_test.dot(u)\n",
    "print('Max:', np.max(score_test), 'Min:', np.min(score_test), \n",
    "      'Mean:', np.mean(score_test), 'Median:', np.median(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 16.876846462594738 Min: -25.125485490338683 Mean: -7.775174116581557 Median: -7.459745580018382\n"
     ]
    }
   ],
   "source": [
    "# score statistics for all the negative windows in the training set\n",
    "X_test = embeddings[neg_windows].reshape(-1, x_dim)\n",
    "z_test = X_test.dot(W) + b\n",
    "a_test = 1. / (1. + np.exp(-z_test))\n",
    "score_test = a_test.dot(u)\n",
    "print('Max:', np.max(score_test), 'Min:', np.min(score_test), \n",
    "      'Mean:', np.mean(score_test), 'Median:', np.median(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
