{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* Stanford CS224n Lecture 4 (Winter 2018) [Slides](https://web.stanford.edu/class/cs224n/lectures/lecture4.pdf)\n",
    "* Stanford CS224n Lecture 4 (Winter 2017) [Video](https://youtu.be/uc2_iwVqrRI)\n",
    "* Denny Britz's post (2015): [Implementing a Neural Network from Scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "from tempfile import gettempdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "print('Python', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Classify whether the center word within a window of words is a location\n",
    "\n",
    "* To build a simple neural network model to illustrate non-linear function approximation, backpropagation and stochastic gradient descent.\n",
    "* Background to the Named Entity Recognition (NER) problem on [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "* Description of task in Stanford CS224n Lecture 4 [slide 45](https://web.stanford.edu/class/cs224n/lectures/lecture4.pdf#page=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data from file\n",
    "Tjong Kim Sang et al. [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition\n",
    "](http://www.aclweb.org/anthology/W03-0419.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, filename, expected_bytes):\n",
    "    \"Download the file if not present, and make sure it's the right size.\"    \n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n",
    "        statinfo = os.stat(local_filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print('Found and verified', filename)\n",
    "        else:\n",
    "            print(statinfo.st_size)\n",
    "            raise Exception('Failed to verify ' + local_filename + \n",
    "                            '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"Reads the eng.train data file from CONLL2003\"\n",
    "    sents, sent_tags = [], []\n",
    "    with open(filename) as f:\n",
    "        dictionary = {'<PAD>': 0}\n",
    "        sent, tags = [], []\n",
    "        for line in f:\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                continue\n",
    "            if line.startswith('\\n'):\n",
    "                if sent and tags:\n",
    "                    sents.append(sent)\n",
    "                    sent_tags.append(tags)\n",
    "                    sent, tags = [], []\n",
    "                continue\n",
    "            word, _, _, tag = line.split()\n",
    "            sent.append(word)\n",
    "            tags.append(tag)\n",
    "            if not dictionary.get(word):\n",
    "                dictionary[word] = len(dictionary)\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return sents, sent_tags, dictionary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = maybe_download(\n",
    "    url='https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/',\n",
    "    filename='eng.train',\n",
    "    expected_bytes=3283420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to view raw text: https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from file and build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sents               : list of sentences (where each sentence is a list of words)\n",
    "    sent_tags           : list of named-entity tags corresponding to each word in sents\n",
    "    dictionary          : maps words(strings) to their IDs(int)\n",
    "    reversed_dictionary : maps IDs(int) to their words(strings)\n",
    "\"\"\"\n",
    "sents, sent_tags, dictionary, reversed_dictionary = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'] ['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('Sample sentence:', sents[0], sent_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23624"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)   # vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word windows for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_windows(window_size=2):\n",
    "    \"\"\"\n",
    "    Param: window_size (int) for each side of center word\n",
    "    Returns: : tuple (list of +ve windows, list of -ve windows)\n",
    "    \"\"\"\n",
    "    pos_windows, neg_windows = [], []\n",
    "    span = 2*window_size + 1\n",
    "    for sent, tags in zip(sents, sent_tags):\n",
    "        count = len(sent)\n",
    "        # pad sentence at front and end\n",
    "        sent = [0]*window_size + [dictionary[word] for word in sent] + [0]*window_size\n",
    "        for i in range(count):\n",
    "            window = sent[i:i+span]\n",
    "            # positive if center word is tagged as location\n",
    "            if tags[i] in ['B-LOC', 'I-LOC']:\n",
    "                pos_windows.append(window)\n",
    "            else:\n",
    "                neg_windows.append(window)\n",
    "    return pos_windows, neg_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive windows:  8297\n",
      "Number of negative windows:  195324\n",
      "Sample positive window:  [0, 0, 12, 13, 0] ['<PAD>', '<PAD>', 'BRUSSELS', '1996-08-22', '<PAD>']\n",
      "Sample negative window:  [0, 0, 1, 2, 3] ['<PAD>', '<PAD>', 'EU', 'rejects', 'German']\n"
     ]
    }
   ],
   "source": [
    "pos_windows, neg_windows = prepare_windows(window_size=2)\n",
    "print('Number of positive windows: ', len(pos_windows))\n",
    "print('Number of negative windows: ', len(neg_windows))\n",
    "print('Sample positive window: ', pos_windows[0], [reversed_dictionary[i] for i in pos_windows[0]])\n",
    "print('Sample negative window: ', neg_windows[0], [reversed_dictionary[i] for i in neg_windows[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameter values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "embedding_size = 100      # word embdedding dimension size \n",
    "window_size = 2           # size of window on each side of center word\n",
    "hidden_size = 200         # size of the hidden layer\n",
    "learning_rate = 0.02     # initial learning rate\n",
    "num_epochs = 100         # number of passes over true window samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test values to monitor training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent = [dictionary[word] for word in 'shops in Paris are amazing'.split()]\n",
    "neg_sent = [dictionary[word] for word in 'not all shops in Paris'.split()]\n",
    "\n",
    "word_ids = np.vstack((pos_sent, neg_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive window (\"shops in Paris are amazing\") score: 0.8836468783513292\n",
      "Negative window (\"not all shops in Paris\") score: -1.256260939044756\n",
      "Epoch 1 error:  0.5579765121942374\n",
      "Epoch 2 error:  0.3331931522768411\n",
      "Epoch 3 error:  0.2909576297149046\n",
      "Epoch 4 error:  0.2541241764738869\n",
      "Epoch 5 error:  0.20039501078979563\n",
      "Epoch 6 error:  0.18689564710487705\n",
      "Epoch 7 error:  0.1707173158075877\n",
      "Epoch 8 error:  0.18102232846116376\n",
      "Epoch 9 error:  0.18015253669296025\n",
      "Positive window (\"shops in Paris are amazing\") score: 0.036454762724530854\n",
      "Negative window (\"not all shops in Paris\") score: -2.8344061881812475\n",
      "Epoch 10 error:  0.1639897789982405\n",
      "Epoch 11 error:  0.15749696569041816\n",
      "Epoch 12 error:  0.13340050849125523\n",
      "Epoch 13 error:  0.12343361510971182\n",
      "Epoch 14 error:  0.12525819434294844\n",
      "Epoch 15 error:  0.12672452132982615\n",
      "Epoch 16 error:  0.13123665753790192\n",
      "Epoch 17 error:  0.13913871885225032\n",
      "Epoch 18 error:  0.11926359214746965\n",
      "Epoch 19 error:  0.11676539673896921\n",
      "Positive window (\"shops in Paris are amazing\") score: 1.1601572149190118\n",
      "Negative window (\"not all shops in Paris\") score: -1.6558697856992248\n",
      "Epoch 20 error:  0.10788836680303866\n",
      "Epoch 21 error:  0.1131415191758626\n",
      "Epoch 22 error:  0.10327887085628658\n",
      "Epoch 23 error:  0.10901662312536294\n",
      "Epoch 24 error:  0.11643447843276059\n",
      "Epoch 25 error:  0.10572291372689333\n",
      "Epoch 26 error:  0.09571313988022227\n",
      "Epoch 27 error:  0.10321178929261479\n",
      "Epoch 28 error:  0.09037345633817218\n",
      "Epoch 29 error:  0.09736554982776639\n",
      "Positive window (\"shops in Paris are amazing\") score: 0.29682560019497484\n",
      "Negative window (\"not all shops in Paris\") score: -2.9677015982525683\n",
      "Epoch 30 error:  0.08344477713959313\n",
      "Epoch 31 error:  0.07723531664869612\n",
      "Epoch 32 error:  0.08111403368820631\n",
      "Epoch 33 error:  0.07576173034453954\n",
      "Epoch 34 error:  0.09213407183309537\n",
      "Epoch 35 error:  0.07782623828354533\n",
      "Epoch 36 error:  0.06722112159974478\n",
      "Epoch 37 error:  0.06835059840675495\n",
      "Epoch 38 error:  0.06591891085433557\n",
      "Epoch 39 error:  0.06037309317176253\n",
      "Positive window (\"shops in Paris are amazing\") score: 0.379964591143752\n",
      "Negative window (\"not all shops in Paris\") score: -4.267942073564971\n",
      "Epoch 40 error:  0.06034122801844783\n",
      "Epoch 41 error:  0.06578967918945436\n",
      "Epoch 42 error:  0.06252849173806803\n",
      "Epoch 43 error:  0.05809218348039746\n",
      "Epoch 44 error:  0.0636965085098807\n",
      "Epoch 45 error:  0.06062451255175055\n",
      "Epoch 46 error:  0.06437285439439787\n",
      "Epoch 47 error:  0.06317307716784325\n",
      "Epoch 48 error:  0.060023019777537046\n",
      "Epoch 49 error:  0.06580793190109693\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.7277908691771061\n",
      "Negative window (\"not all shops in Paris\") score: -3.521615597919592\n",
      "Epoch 50 error:  0.06004483643650668\n",
      "Epoch 51 error:  0.058478411258373726\n",
      "Epoch 52 error:  0.059988618582090925\n",
      "Epoch 53 error:  0.06485545817324148\n",
      "Epoch 54 error:  0.06464379235181164\n",
      "Epoch 55 error:  0.06357331243033923\n",
      "Epoch 56 error:  0.06260410819518963\n",
      "Epoch 57 error:  0.06434744226977966\n",
      "Epoch 58 error:  0.06582370617221406\n",
      "Epoch 59 error:  0.06251383754245111\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.3489761237409037\n",
      "Negative window (\"not all shops in Paris\") score: -3.405865602033373\n",
      "Epoch 60 error:  0.060488023358058\n",
      "Epoch 61 error:  0.0632441460393834\n",
      "Epoch 62 error:  0.05267379626078868\n",
      "Epoch 63 error:  0.056053527061238\n",
      "Epoch 64 error:  0.05191357145200376\n",
      "Epoch 65 error:  0.05516212272897736\n",
      "Epoch 66 error:  0.060151216561514544\n",
      "Epoch 67 error:  0.06401618482347772\n",
      "Epoch 68 error:  0.058912863749572525\n",
      "Epoch 69 error:  0.07090579625436963\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.5242894722609825\n",
      "Negative window (\"not all shops in Paris\") score: -4.370476601210726\n",
      "Epoch 70 error:  0.065384331447549\n",
      "Epoch 71 error:  0.05131224069215181\n",
      "Epoch 72 error:  0.056018828078257754\n",
      "Epoch 73 error:  0.050379686141154095\n",
      "Epoch 74 error:  0.05304673505771583\n",
      "Epoch 75 error:  0.05235482730126475\n",
      "Epoch 76 error:  0.05421204399877283\n",
      "Epoch 77 error:  0.053775009726184785\n",
      "Epoch 78 error:  0.06101121508285043\n",
      "Epoch 79 error:  0.05455032005562602\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.07642871322414568\n",
      "Negative window (\"not all shops in Paris\") score: -5.186419516240174\n",
      "Epoch 80 error:  0.05274879607278613\n",
      "Epoch 81 error:  0.05294049911075941\n",
      "Epoch 82 error:  0.05044230529587058\n",
      "Epoch 83 error:  0.04884514134959271\n",
      "Epoch 84 error:  0.04703379394372657\n",
      "Epoch 85 error:  0.045150336202251944\n",
      "Epoch 86 error:  0.04118081352999633\n",
      "Epoch 87 error:  0.05098485786226943\n",
      "Epoch 88 error:  0.049763960615449264\n",
      "Epoch 89 error:  0.05058930802607\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.14515371524204734\n",
      "Negative window (\"not all shops in Paris\") score: -3.4011576909585615\n",
      "Epoch 90 error:  0.0485288187878872\n",
      "Epoch 91 error:  0.04622590995806391\n",
      "Epoch 92 error:  0.04682003894816414\n",
      "Epoch 93 error:  0.04192704070384504\n",
      "Epoch 94 error:  0.04544777906305949\n",
      "Epoch 95 error:  0.04491025928471803\n",
      "Epoch 96 error:  0.04784299471942321\n",
      "Epoch 97 error:  0.03918155863515364\n",
      "Epoch 98 error:  0.044115905707219315\n",
      "Epoch 99 error:  0.04319469387308519\n",
      "Positive window (\"shops in Paris are amazing\") score: 0.7013054034031763\n",
      "Negative window (\"not all shops in Paris\") score: -3.2119035610006774\n",
      "Epoch 100 error:  0.04137427989565557\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This code trains a simple neural network as a binary classifier. \n",
    "    The model calculates a score when it is given a window of words. \n",
    "    The score is used to determine whether the center word in the\n",
    "    window is a location or not.\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(seed)\n",
    "vocab_size = len(dictionary)\n",
    "x_dim = embedding_size * (2*window_size + 1)\n",
    "\n",
    "# Initialize model parameters \n",
    "embeddings = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_size))\n",
    "W = np.random.randn(x_dim, hidden_size) * np.sqrt(1.0/x_dim)\n",
    "b = np.zeros(hidden_size)\n",
    "u = np.random.randn(hidden_size)\n",
    "\n",
    "average_error = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, pos_window in enumerate(pos_windows):\n",
    "        neg_window = random.sample(neg_windows, k=1) # s_c\n",
    "        inputs = np.vstack((pos_window, neg_window)) # stack as matrix\n",
    "        X = embeddings[inputs].reshape(-1, x_dim)    # concat the words\n",
    "        \n",
    "        # Forward pass    \n",
    "        z = X.dot(W) + b                # affine transformation\n",
    "        a = 1. / (1. + np.exp(-z))      # non-linearity (sigmoid)\n",
    "        scores = a.dot(u)               # scalar unnormalized scores\n",
    "\n",
    "        # Max-margin objective\n",
    "        error = 1 if max(0, 1 - scores[0] + scores[1]) > 0 else 0\n",
    "        \n",
    "        # Backward pass (no updating if error is 0)\n",
    "        grad_u = error * (a[1] - a[0])       # gradient for u\n",
    "        delta = grad_u.dot(u) * (a*(1 - a))  # multiply with sigmoid derivative\n",
    "        grad_W = X.T.dot(delta)              # gradient for W\n",
    "        grad_b = delta.sum(axis=0)           # gradient for b\n",
    "        grad_X = delta.dot(W.T)              # gradient for the word vectors\n",
    "        grad_X = grad_X.reshape(-1, 2*window_size + 1, embedding_size)                                \n",
    "        \n",
    "        # Parameter updates using gradient descent\n",
    "        u -= learning_rate * grad_u\n",
    "        W -= learning_rate * grad_W\n",
    "        b -= learning_rate * grad_b\n",
    "        embeddings[inputs] -= learning_rate * grad_X\n",
    "        \n",
    "        # Keep track of any errors\n",
    "        if error: average_error += 1 - scores[0] + scores[1]\n",
    "         \n",
    "    # Check scores for test pair\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        X_test = embeddings[word_ids].reshape(-1, x_dim)\n",
    "        z_test = X_test.dot(W) + b\n",
    "        a_test = 1. / (1. + np.exp(-z_test))\n",
    "        scores_test = a_test.dot(u)\n",
    "        print('Positive window (\"shops in Paris are amazing\") score:', scores_test[0])\n",
    "        print('Negative window (\"not all shops in Paris\") score:', scores_test[1])\n",
    "\n",
    "    # Print average error per epoch\n",
    "    print('Epoch', epoch + 1, 'error: ', average_error / i)\n",
    "    \n",
    "    # Stop training when average error is low enough\n",
    "    if average_error / i < 0.02:\n",
    "        break\n",
    "    \n",
    "    average_error = 0\n",
    "        \n",
    "    # Decay learning rate exponentially every epoch\n",
    "    learning_rate = learning_rate * 0.9999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 8.254149662257406 Min: -5.2024479942477635 Mean: 1.1503232412480688 Median: 0.7110955563133758\n"
     ]
    }
   ],
   "source": [
    "# score statistics for all the positive windows in the training set\n",
    "X_test = embeddings[pos_windows].reshape(-1, x_dim)\n",
    "z_test = X_test.dot(W) + b\n",
    "a_test = 1. / (1. + np.exp(-z_test))\n",
    "score_test = a_test.dot(u)\n",
    "print('Max:', np.max(score_test), 'Min:', np.min(score_test), \n",
    "      'Mean:', np.mean(score_test), 'Median:', np.median(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 2.6992802260686886 Min: -78.50381807390244 Mean: -11.558772555719747 Median: -8.607182845892048\n"
     ]
    }
   ],
   "source": [
    "# score statistics for all the negative windows in the training set\n",
    "X_test = embeddings[neg_windows].reshape(-1, x_dim)\n",
    "z_test = X_test.dot(W) + b\n",
    "a_test = 1. / (1. + np.exp(-z_test))\n",
    "score_test = a_test.dot(u)\n",
    "print('Max:', np.max(score_test), 'Min:', np.min(score_test), \n",
    "      'Mean:', np.mean(score_test), 'Median:', np.median(score_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
