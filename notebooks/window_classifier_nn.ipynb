{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* Stanford CS224n Lecture 4 (Winter 2018) [Slides](https://web.stanford.edu/class/cs224n/lectures/lecture4.pdf)\n",
    "* Stanford CS224n Lecture 4 (Winter 2017) [Video](https://youtu.be/uc2_iwVqrRI)\n",
    "* Denny Britz's post (2015): [Implementing a Neural Network from Scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "from tempfile import gettempdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "print('Python', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Classify whether the center word within a window of words is a location\n",
    "\n",
    "* To build a simple neural network model to illustrate non-linear function approximation, backpropagation and stochastic gradient descent.\n",
    "* Background to the Named Entity Recognition (NER) problem on [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "* Description of task in Stanford CS224n Lecture 4 [slide 45](https://web.stanford.edu/class/cs224n/lectures/lecture4.pdf#page=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data from file\n",
    "Tjong Kim Sang et al. [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition\n",
    "](http://www.aclweb.org/anthology/W03-0419.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, filename, expected_bytes):\n",
    "    \"Download the file if not present, and make sure it's the right size.\"    \n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n",
    "        statinfo = os.stat(local_filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print('Found and verified', filename)\n",
    "        else:\n",
    "            print(statinfo.st_size)\n",
    "            raise Exception('Failed to verify ' + local_filename + \n",
    "                            '. Can you get to it with a browser?')\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"Reads the eng.train data file from CONLL2003\"\n",
    "    sents, sent_tags = [], []\n",
    "    with open(filename) as f:\n",
    "        dictionary = {'<PAD>': 0}\n",
    "        sent, tags = [], []\n",
    "        for line in f:\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                continue\n",
    "            if line.startswith('\\n'):\n",
    "                if sent and tags:\n",
    "                    sents.append(sent)\n",
    "                    sent_tags.append(tags)\n",
    "                    sent, tags = [], []\n",
    "                continue\n",
    "            word, _, _, tag = line.split()\n",
    "            sent.append(word)\n",
    "            tags.append(tag)\n",
    "            if not dictionary.get(word):\n",
    "                dictionary[word] = len(dictionary)\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return sents, sent_tags, dictionary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = maybe_download(\n",
    "    url='https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/',\n",
    "    filename='eng.train',\n",
    "    expected_bytes=3283420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click to view raw text: https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from file and build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sents               : list of sentences (where each sentence is a list of words)\n",
    "    sent_tags           : list of named-entity tags corresponding to each word in sents\n",
    "    dictionary          : maps words(strings) to their IDs(int)\n",
    "    reversed_dictionary : maps IDs(int) to their words(strings)\n",
    "\"\"\"\n",
    "sents, sent_tags, dictionary, reversed_dictionary = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'] ['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('Sample sentence:', sents[0], sent_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23624"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)   # vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word windows for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_windows(window_size=2):\n",
    "    \"\"\"\n",
    "    Param: window_size (int) for each side of center word\n",
    "    Returns: : tuple (list of +ve windows, list of -ve windows)\n",
    "    \"\"\"\n",
    "    pos_windows, neg_windows = [], []\n",
    "    span = 2*window_size + 1\n",
    "    for sent, tags in zip(sents, sent_tags):\n",
    "        count = len(sent)\n",
    "        # pad sentence at front and end\n",
    "        sent = [0]*window_size + [dictionary[word] for word in sent] + [0]*window_size\n",
    "        for i in range(count):\n",
    "            window = sent[i:i+span]\n",
    "            # positive if center word is tagged as location\n",
    "            if tags[i] in ['B-LOC', 'I-LOC']:\n",
    "                pos_windows.append(window)\n",
    "            else:\n",
    "                neg_windows.append(window)\n",
    "    return pos_windows, neg_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive windows:  8297\n",
      "Number of negative windows:  195324\n",
      "Sample positive window:  [0, 0, 12, 13, 0] ['<PAD>', '<PAD>', 'BRUSSELS', '1996-08-22', '<PAD>']\n",
      "Sample negative window:  [0, 0, 1, 2, 3] ['<PAD>', '<PAD>', 'EU', 'rejects', 'German']\n"
     ]
    }
   ],
   "source": [
    "pos_windows, neg_windows = prepare_windows(window_size=2)\n",
    "print('Number of positive windows: ', len(pos_windows))\n",
    "print('Number of negative windows: ', len(neg_windows))\n",
    "print('Sample positive window: ', pos_windows[0], [reversed_dictionary[i] for i in pos_windows[0]])\n",
    "print('Sample negative window: ', neg_windows[0], [reversed_dictionary[i] for i in neg_windows[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameter values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "embedding_size = 100      # word embdedding dimension size \n",
    "window_size = 2           # size of window on each side of center word\n",
    "hidden_size = 200         # size of the hidden layer\n",
    "vocab_size = len(dictionary)\n",
    "learning_rate = 0.02     # initial learning rate\n",
    "num_epochs = 50         # number of passes over true window samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pair to monitor training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent = [dictionary[word] for word in 'shops in Paris are amazing'.split()]\n",
    "neg_sent = [dictionary[word] for word in 'not all shops in Paris'.split()]\n",
    "\n",
    "word_ids = np.vstack((pos_sent, neg_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive window (\"shops in Paris are amazing\") score: -1.273931665291056\n",
      "Negative window (\"not all shops in Paris\") score: -1.5168229579388717\n",
      "Epoch 1 error:  0.5681694585376684\n",
      "Epoch 2 error:  0.3517308625707598\n",
      "Epoch 3 error:  0.29509657292216906\n",
      "Epoch 4 error:  0.27626718095696684\n",
      "Epoch 5 error:  0.271596474768546\n",
      "Epoch 6 error:  0.2639316845939885\n",
      "Epoch 7 error:  0.24087400866000877\n",
      "Epoch 8 error:  0.24022748521970655\n",
      "Epoch 9 error:  0.24200274709608163\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.3831977708831287\n",
      "Negative window (\"not all shops in Paris\") score: -3.355656787063505\n",
      "Epoch 10 error:  0.21636457221406583\n",
      "Epoch 11 error:  0.2193500204942525\n",
      "Epoch 12 error:  0.21719799476115917\n",
      "Epoch 13 error:  0.21710344196397818\n",
      "Epoch 14 error:  0.18065922147745456\n",
      "Epoch 15 error:  0.17499168725680908\n",
      "Epoch 16 error:  0.1730905437622126\n",
      "Epoch 17 error:  0.16618494735896303\n",
      "Epoch 18 error:  0.17323967390513403\n",
      "Epoch 19 error:  0.16541743160806385\n",
      "Positive window (\"shops in Paris are amazing\") score: 0.5954383418530182\n",
      "Negative window (\"not all shops in Paris\") score: -1.2596908056740772\n",
      "Epoch 20 error:  0.15608948429541153\n",
      "Epoch 21 error:  0.1614469934168529\n",
      "Epoch 22 error:  0.15731852135134816\n",
      "Epoch 23 error:  0.156203173956683\n",
      "Epoch 24 error:  0.15864287775125369\n",
      "Epoch 25 error:  0.14130098863799262\n",
      "Epoch 26 error:  0.13667980966476956\n",
      "Epoch 27 error:  0.1261711798178326\n",
      "Epoch 28 error:  0.1353677592437575\n",
      "Epoch 29 error:  0.12707655264928447\n",
      "Positive window (\"shops in Paris are amazing\") score: 0.42691380215182917\n",
      "Negative window (\"not all shops in Paris\") score: -6.616736835608565\n",
      "Epoch 30 error:  0.11361646327483317\n",
      "Epoch 31 error:  0.11309052795676358\n",
      "Epoch 32 error:  0.12278857737699955\n",
      "Epoch 33 error:  0.1134717493350741\n",
      "Epoch 34 error:  0.1133178760125569\n",
      "Epoch 35 error:  0.11259904594405362\n",
      "Epoch 36 error:  0.1316800021381946\n",
      "Epoch 37 error:  0.12301176459851397\n",
      "Epoch 38 error:  0.10912131542906954\n",
      "Epoch 39 error:  0.10793165890303545\n",
      "Positive window (\"shops in Paris are amazing\") score: 0.5825017029888355\n",
      "Negative window (\"not all shops in Paris\") score: -4.546764407462513\n",
      "Epoch 40 error:  0.10510524629286296\n",
      "Epoch 41 error:  0.09850070527242044\n",
      "Epoch 42 error:  0.09163871048475797\n",
      "Epoch 43 error:  0.09215960948529209\n",
      "Epoch 44 error:  0.08471412961561212\n",
      "Epoch 45 error:  0.0778336717462449\n",
      "Epoch 46 error:  0.07569594476887033\n",
      "Epoch 47 error:  0.07387406746933227\n",
      "Epoch 48 error:  0.07266883842556096\n",
      "Epoch 49 error:  0.06595298880483579\n",
      "Positive window (\"shops in Paris are amazing\") score: -0.034971158035953465\n",
      "Negative window (\"not all shops in Paris\") score: -7.941008614668316\n",
      "Epoch 50 error:  0.07370245614904335\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This code trains a simple neural network as a binary classifier. \n",
    "    The model calculates a score when it is given a window of words. \n",
    "    The score is used to determine whether the center word in the\n",
    "    window is a location or not.\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(seed)\n",
    "x_dim = embedding_size * (2*window_size + 1)\n",
    "\n",
    "# Initialize model parameters \n",
    "embeddings = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_size))\n",
    "W = np.random.randn(x_dim, hidden_size) * np.sqrt(1.0/x_dim)\n",
    "b = np.zeros(hidden_size)\n",
    "u = np.random.randn(hidden_size)\n",
    "\n",
    "average_error = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, pos_window in enumerate(pos_windows):\n",
    "        neg_window = random.sample(neg_windows, k=1) # s_c\n",
    "        inputs = np.vstack((pos_window, neg_window)) # stack as matrix\n",
    "        X = embeddings[inputs].reshape(-1, x_dim)    # concat the words\n",
    "        \n",
    "        # Forward pass    \n",
    "        z = X.dot(W) + b                # affine transformation\n",
    "        a = 1. / (1. + np.exp(-z))      # non-linearity (sigmoid)\n",
    "        scores = a.dot(u)               # scalar unnormalized scores\n",
    "\n",
    "        # Max-margin objective\n",
    "        error = 1 if max(0, 1 - scores[0] + scores[1]) > 0 else 0\n",
    "        \n",
    "        # Backward pass (does updates only if error==1)\n",
    "        # Risk of catastrophic forgetting after many epochs?\n",
    "        grad_u = error * (a[1] - a[0])       # gradient for u\n",
    "        delta = grad_u.dot(u) * (a*(1 - a))  # multiply with sigmoid derivative\n",
    "        grad_W = X.T.dot(delta)              # gradient for W\n",
    "        grad_b = delta.sum(axis=0)           # gradient for b\n",
    "        grad_X = delta.dot(W.T)              # gradient for the word vectors\n",
    "        grad_X = grad_X.reshape(-1, 2*window_size + 1, embedding_size)                                \n",
    "        \n",
    "        # Parameter updates using gradient descent\n",
    "        u -= learning_rate * grad_u\n",
    "        W -= learning_rate * grad_W\n",
    "        b -= learning_rate * grad_b\n",
    "        embeddings[inputs] -= learning_rate * grad_X\n",
    "        \n",
    "        # Keep track of any errors\n",
    "        if error: average_error += 1 - scores[0] + scores[1]\n",
    "         \n",
    "    # Check scores for test pair\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        X_test = embeddings[word_ids].reshape(-1, x_dim)\n",
    "        z_test = X_test.dot(W) + b\n",
    "        a_test = 1. / (1. + np.exp(-z_test))\n",
    "        scores_test = a_test.dot(u)\n",
    "        print('Positive window (\"shops in Paris are amazing\") score:', scores_test[0])\n",
    "        print('Negative window (\"not all shops in Paris\") score:', scores_test[1])\n",
    "\n",
    "    # Print average error per epoch\n",
    "    print('Epoch', epoch + 1, 'error: ', average_error / i)\n",
    "    \n",
    "    # Stop training when average error is low enough\n",
    "    if average_error / i < 0.05:\n",
    "        break\n",
    "    \n",
    "    average_error = 0\n",
    "        \n",
    "    # Decay learning rate exponentially every epoch\n",
    "    learning_rate = learning_rate * 0.9999\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 7.38757692414721 Min: -4.791769095301732 Mean: 0.6754364712490354 Median: 0.16118231667727145\n"
     ]
    }
   ],
   "source": [
    "# score statistics for all the positive windows in the training set\n",
    "X_test = embeddings[pos_windows].reshape(-1, x_dim)\n",
    "z_test = X_test.dot(W) + b\n",
    "a_test = 1. / (1. + np.exp(-z_test))\n",
    "score_test = a_test.dot(u)\n",
    "print('Max:', np.max(score_test), 'Min:', np.min(score_test), \n",
    "      'Mean:', np.mean(score_test), 'Median:', np.median(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 4.561646784424768 Min: -102.16381260117203 Mean: -13.970463366480589 Median: -7.630605513695681\n"
     ]
    }
   ],
   "source": [
    "# score statistics for all the negative windows in the training set\n",
    "X_test = embeddings[neg_windows].reshape(-1, x_dim)\n",
    "z_test = X_test.dot(W) + b\n",
    "a_test = 1. / (1. + np.exp(-z_test))\n",
    "score_test = a_test.dot(u)\n",
    "print('Max:', np.max(score_test), 'Min:', np.min(score_test), \n",
    "      'Mean:', np.mean(score_test), 'Median:', np.median(score_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
